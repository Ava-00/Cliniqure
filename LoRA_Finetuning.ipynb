{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yavPNHSFr_Ov",
        "outputId": "0a0088e2-12d5-4cb7-9a7c-6ac8cd3f15a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.system(\"pip install transformers accelerate datasets peft\")\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
        "from peft import get_peft_model, LoraConfig\n",
        "from datasets import load_dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"FreedomIntelligence/Disease_Database\", 'en')\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_name = \"tiiuae/falcon-7b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# Encode labels using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "labels = dataset[\"train\"][\"disease\"]\n",
        "label_encoder.fit(labels)\n",
        "# Define the number of labels\n",
        "num_labels = len(label_encoder.classes_)\n",
        "print(f\"Number of labels: {num_labels}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model and adjust for classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "# Add padding token to tokenizer\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "tokenizer.pad_token = '[PAD]'\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "actual_vocab_size = len(tokenizer.get_vocab())\n",
        "print(\"Actual vocab size (with special tokens):\", actual_vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "ycJusgXfy5PS",
        "outputId": "0281ad68-fa76-4f5a-fc09-b5ce70921a5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AutoModelForSequenceClassification' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-671ff43d7698>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the pre-trained model and adjust for classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Add padding token to tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'pad_token'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'[PAD]'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForSequenceClassification' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    tokenized = tokenizer(examples['common_symptom'], truncation=True, padding=\"max_length\")\n",
        "    tokenized[\"labels\"] = label_encoder.transform(examples['disease'])\n",
        "    return tokenized\n",
        "\n",
        "# Split dataset into training and evaluation sets\n",
        "split_dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
        "train_dataset = split_dataset[\"train\"].map(preprocess_function, batched=True)\n",
        "eval_dataset = split_dataset[\"test\"].map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "ub-lGZFizDxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\"  # Task is sequence classification\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=\"./logs\",\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=torch.cuda.is_available(),  # Use mixed precision if supported\n",
        "    logging_steps=10\n",
        ")"
      ],
      "metadata": {
        "id": "ZgoodkD9zEzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model and tokenizer\n",
        "model.save_pretrained(\"./falcon_finetuned\")\n",
        "tokenizer.save_pretrained(\"./falcon_finetuned\")\n",
        "\n",
        "# Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "print(\"Evaluation Results:\", results)\n"
      ],
      "metadata": {
        "id": "62gaW-r4zNKl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}